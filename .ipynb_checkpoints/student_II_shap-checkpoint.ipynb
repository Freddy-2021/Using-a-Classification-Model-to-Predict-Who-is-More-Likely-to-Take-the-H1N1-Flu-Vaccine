{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](images/pexels-pixabay-40568.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Project\n",
    "\n",
    "**Author:** Freddy Abrahamson<br>\n",
    "**Date created:** 3-27-2022<br>\n",
    "**Discipline:** Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "For this project, I will use multiple linear regression modeling to analyze house sales in King County, in Washington state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "The goal of this project is to to provide advice to homeowners about how home renovations can increase the value of their homes, and by what amount. The information for this project is derived from information comprised of the different characteristics of over 20,000 homes in King County,which is located in Washington State. I will use this information gain a better understanding about how different remodels, or renovations to the homes listed, impact their price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Describe the data being used for this project.\n",
    "***\n",
    "The data comes from the King County House Sales dataset, in the form of a 'csv' file. The file will be converted into a pandas dataframe. It contains information about the different characteristics of the homes in the King County area,including the number of bedrooms, building grades, square footage, and price. King County is located in Washington State, and has a size of approximately 2300 square miles, per the U.S Census Bureau:\n",
    "\n",
    "kc_house_data.csv\n",
    "\n",
    "\n",
    "I will be giving this dataframe a brief overview of its different characteristics, with a view toward using its columns as variables in a regression model. These include:\n",
    "\n",
    "* dataframe shape: the number of rows and columns in the dataframe\n",
    "* any missing/null values\n",
    "* continuous variables\n",
    "* categorical variables\n",
    "* binary variables\n",
    "* zero inflated variables\n",
    "* outliers\n",
    "\n",
    "Since the goal is to try to gain insights, as to how much much a particular upgrade or remodel can the impact the\n",
    "price of the house, as opposed to predicting home prices, I will be placing an emphasis on choosing features with the least explanatory overlap. To that end, for instance, I would favor a feature such as a bedroom, or a bathroom over square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_metric(df1, df2):\n",
    "    from sklearn.metrics import auc\n",
    "    \n",
    "    rows = len(df1)\n",
    "    test_scores = np.zeros((rows, 3))\n",
    "    score_diffs = np.zeros((rows, 3))\n",
    "    auc_scores = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        test_scores[row][1] = df1['mean_test_score'][row]\n",
    "        test_scores[row][2] = 1\n",
    "        score_diffs[row][1] = df1['score_dif'][row]\n",
    "        score_diffs[row][2] = 1\n",
    "    \n",
    "    for row in range(rows):\n",
    "        auc_score = auc(score_diffs[row], test_scores[row])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "    best_auc_score = max(auc_scores)\n",
    "    best_score_index = auc_scores.index(best_auc_score)\n",
    "    return (df1['mean_test_score'][best_score_index], df1['score_dif'][best_score_index], best_auc_score,\n",
    "            df2['params'][best_score_index],best_score_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "df = pd.read_csv('H1N1_Flu_Vaccines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26707 entries, 0 to 26706\n",
      "Data columns (total 38 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   respondent_id                26707 non-null  int64  \n",
      " 1   h1n1_concern                 26615 non-null  float64\n",
      " 2   h1n1_knowledge               26591 non-null  float64\n",
      " 3   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 4   behavioral_avoidance         26499 non-null  float64\n",
      " 5   behavioral_face_mask         26688 non-null  float64\n",
      " 6   behavioral_wash_hands        26665 non-null  float64\n",
      " 7   behavioral_large_gatherings  26620 non-null  float64\n",
      " 8   behavioral_outside_home      26625 non-null  float64\n",
      " 9   behavioral_touch_face        26579 non-null  float64\n",
      " 10  doctor_recc_h1n1             24547 non-null  float64\n",
      " 11  doctor_recc_seasonal         24547 non-null  float64\n",
      " 12  chronic_med_condition        25736 non-null  float64\n",
      " 13  child_under_6_months         25887 non-null  float64\n",
      " 14  health_worker                25903 non-null  float64\n",
      " 15  health_insurance             14433 non-null  float64\n",
      " 16  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 17  opinion_h1n1_risk            26319 non-null  float64\n",
      " 18  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 19  opinion_seas_vacc_effective  26245 non-null  float64\n",
      " 20  opinion_seas_risk            26193 non-null  float64\n",
      " 21  opinion_seas_sick_from_vacc  26170 non-null  float64\n",
      " 22  age_group                    26707 non-null  object \n",
      " 23  education                    25300 non-null  object \n",
      " 24  race                         26707 non-null  object \n",
      " 25  sex                          26707 non-null  object \n",
      " 26  income_poverty               22284 non-null  object \n",
      " 27  marital_status               25299 non-null  object \n",
      " 28  rent_or_own                  24665 non-null  object \n",
      " 29  employment_status            25244 non-null  object \n",
      " 30  hhs_geo_region               26707 non-null  object \n",
      " 31  census_msa                   26707 non-null  object \n",
      " 32  household_adults             26458 non-null  float64\n",
      " 33  household_children           26458 non-null  float64\n",
      " 34  employment_industry          13377 non-null  object \n",
      " 35  employment_occupation        13237 non-null  object \n",
      " 36  h1n1_vaccine                 26707 non-null  int64  \n",
      " 37  seasonal_vaccine             26707 non-null  int64  \n",
      "dtypes: float64(23), int64(3), object(12)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "      <th>seasonal_vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...  rent_or_own   employment_status  \\\n",
       "0                    1.0  ...          Own  Not in Labor Force   \n",
       "1                    1.0  ...         Rent            Employed   \n",
       "2                    0.0  ...          Own            Employed   \n",
       "3                    0.0  ...         Rent  Not in Labor Force   \n",
       "4                    1.0  ...          Own            Employed   \n",
       "\n",
       "   hhs_geo_region                census_msa  household_adults  \\\n",
       "0        oxchjgsf                   Non-MSA               0.0   \n",
       "1        bhuqouqj  MSA, Not Principle  City               0.0   \n",
       "2        qufhixun  MSA, Not Principle  City               2.0   \n",
       "3        lrircsnp       MSA, Principle City               0.0   \n",
       "4        qufhixun  MSA, Not Principle  City               1.0   \n",
       "\n",
       "   household_children  employment_industry  employment_occupation  \\\n",
       "0                 0.0                  NaN                    NaN   \n",
       "1                 0.0             pxcmvdjn               xgwztkwe   \n",
       "2                 0.0             rucpziij               xtkaffoo   \n",
       "3                 0.0                  NaN                    NaN   \n",
       "4                 0.0             wxleyezf               emcorrxb   \n",
       "\n",
       "   h1n1_vaccine  seasonal_vaccine  \n",
       "0             0                 0  \n",
       "1             0                 1  \n",
       "2             0                 0  \n",
       "3             0                 1  \n",
       "4             0                 0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Counts\n",
      "0    21033\n",
      "1     5674\n",
      "Name: h1n1_vaccine, dtype: int64\n",
      "\n",
      "Percentages\n",
      "0    0.787546\n",
      "1    0.212454\n",
      "Name: h1n1_vaccine, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Counts\")\n",
    "print(df[\"h1n1_vaccine\"].value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(df[\"h1n1_vaccine\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A baseline model that always chose the majority class would have an accuracy of over 78%.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Features, Train-test-split, and Dealing with Missing Values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I will drop:\n",
    "# 'respondent_id' - since it is a unique identifier\n",
    "# 'employment_industry','employment_occupation','health_insurance' - about 50% or more records missing \n",
    "# 'seasonal_vaccine' - we will not account for this classification\n",
    "df_II = df.drop(['respondent_id','employment_industry','employment_occupation','health_insurance'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into X and y\n",
    "X = df_II.drop(\"h1n1_vaccine\", axis=1)\n",
    "y = df_II[\"h1n1_vaccine\"]\n",
    "\n",
    "# Perform train-test split with random_state=42 and stratify=y\n",
    "# stratify y to maintain uniform ratios of dependent variable y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#impute values based on most common value in each column:\n",
    "X_train = X_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "X_test = X_test.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There is now no missing data in the training dataset.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>opinion_h1n1_vacc_effective</th>\n",
       "      <th>opinion_h1n1_risk</th>\n",
       "      <th>opinion_h1n1_sick_from_vacc</th>\n",
       "      <th>opinion_seas_vacc_effective</th>\n",
       "      <th>opinion_seas_risk</th>\n",
       "      <th>opinion_seas_sick_from_vacc</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>...</th>\n",
       "      <th>hhs_geo_region_mlyzmhmf</th>\n",
       "      <th>hhs_geo_region_oxchjgsf</th>\n",
       "      <th>hhs_geo_region_qufhixun</th>\n",
       "      <th>census_msa_MSA, Not Principle  City</th>\n",
       "      <th>census_msa_MSA, Principle City</th>\n",
       "      <th>census_msa_Non-MSA</th>\n",
       "      <th>seasonal_vaccine_0</th>\n",
       "      <th>seasonal_vaccine_1</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11075</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16691</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       h1n1_concern  h1n1_knowledge  opinion_h1n1_vacc_effective  \\\n",
       "11075           2.0             2.0                          3.0   \n",
       "7807            3.0             2.0                          3.0   \n",
       "3014            2.0             1.0                          4.0   \n",
       "1671            2.0             1.0                          2.0   \n",
       "16691           3.0             2.0                          4.0   \n",
       "\n",
       "       opinion_h1n1_risk  opinion_h1n1_sick_from_vacc  \\\n",
       "11075                1.0                          1.0   \n",
       "7807                 1.0                          4.0   \n",
       "3014                 0.0                          0.0   \n",
       "1671                 4.0                          3.0   \n",
       "16691                1.0                          3.0   \n",
       "\n",
       "       opinion_seas_vacc_effective  opinion_seas_risk  \\\n",
       "11075                          3.0                1.0   \n",
       "7807                           3.0                1.0   \n",
       "3014                           4.0                0.0   \n",
       "1671                           3.0                4.0   \n",
       "16691                          4.0                1.0   \n",
       "\n",
       "       opinion_seas_sick_from_vacc  age_group  education  ...  \\\n",
       "11075                          3.0        1.0        3.0  ...   \n",
       "7807                           3.0        1.0        2.0  ...   \n",
       "3014                           0.0        4.0        0.0  ...   \n",
       "1671                           3.0        0.0        0.0  ...   \n",
       "16691                          3.0        4.0        2.0  ...   \n",
       "\n",
       "       hhs_geo_region_mlyzmhmf  hhs_geo_region_oxchjgsf  \\\n",
       "11075                      0.0                      0.0   \n",
       "7807                       0.0                      0.0   \n",
       "3014                       0.0                      0.0   \n",
       "1671                       0.0                      1.0   \n",
       "16691                      0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_qufhixun  census_msa_MSA, Not Principle  City  \\\n",
       "11075                      1.0                                  1.0   \n",
       "7807                       0.0                                  0.0   \n",
       "3014                       0.0                                  0.0   \n",
       "1671                       0.0                                  0.0   \n",
       "16691                      0.0                                  1.0   \n",
       "\n",
       "       census_msa_MSA, Principle City  census_msa_Non-MSA  seasonal_vaccine_0  \\\n",
       "11075                             0.0                 0.0                 1.0   \n",
       "7807                              1.0                 0.0                 1.0   \n",
       "3014                              0.0                 1.0                 1.0   \n",
       "1671                              0.0                 1.0                 1.0   \n",
       "16691                             0.0                 0.0                 0.0   \n",
       "\n",
       "       seasonal_vaccine_1  household_adults  household_children  \n",
       "11075                 0.0               1.0                 2.0  \n",
       "7807                  0.0               1.0                 0.0  \n",
       "3014                  0.0               1.0                 0.0  \n",
       "1671                  0.0               0.0                 0.0  \n",
       "16691                 1.0               1.0                 0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting dataframe between ordinals , categoricals, and nominals\n",
    "X_train_ord =  X_train.iloc[:,np.r_[0:2,14:22,24]]\n",
    "X_train_nom = X_train.iloc[:,30:32]\n",
    "ord_cols = X_train_ord.columns\n",
    "nom_cols = X_train_nom.columns\n",
    "cols_to_drop = ord_cols.append(nom_cols)\n",
    "X_train_cat = X_train.drop(cols_to_drop, axis=1)\n",
    "X_train_ord_index = X_train_ord.index\n",
    "X_train_cat_index = X_train_cat.index\n",
    "\n",
    "# I will convert all the columns in the dataset to string type, so I can then encode them:\n",
    "X_train_ord = X_train_ord.astype(str)\n",
    "X_train_cat = X_train_cat.astype(str)\n",
    "\n",
    "# creating a Encoder objects:\n",
    "enc = OrdinalEncoder()\n",
    "ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# fitting dataset to OneHotEncoder object:\n",
    "X_train_enc = enc.fit_transform(X_train_ord)\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "\n",
    "# creating an array with enc and ohe column names:\n",
    "enc_col_names = X_train_ord.columns\n",
    "ohe_col_names = ohe.get_feature_names(X_train_cat.columns)\n",
    "\n",
    "# Setting arrays back to dataframes\n",
    "X_train_enc_df = pd.DataFrame(X_train_enc, columns=enc_col_names,index=X_train_ord_index)\n",
    "X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=ohe_col_names,index=X_train_cat_index)\n",
    "\n",
    "#putting the datframe back together:\n",
    "X_train_II_encoded = pd.concat([X_train_enc_df,X_train_ohe_df,X_train_nom],axis=1)\n",
    "X_train_II_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>opinion_h1n1_vacc_effective</th>\n",
       "      <th>opinion_h1n1_risk</th>\n",
       "      <th>opinion_h1n1_sick_from_vacc</th>\n",
       "      <th>opinion_seas_vacc_effective</th>\n",
       "      <th>opinion_seas_risk</th>\n",
       "      <th>opinion_seas_sick_from_vacc</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>...</th>\n",
       "      <th>hhs_geo_region_mlyzmhmf</th>\n",
       "      <th>hhs_geo_region_oxchjgsf</th>\n",
       "      <th>hhs_geo_region_qufhixun</th>\n",
       "      <th>census_msa_MSA, Not Principle  City</th>\n",
       "      <th>census_msa_MSA, Principle City</th>\n",
       "      <th>census_msa_Non-MSA</th>\n",
       "      <th>seasonal_vaccine_0</th>\n",
       "      <th>seasonal_vaccine_1</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12369</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17593</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13754</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7106</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       h1n1_concern  h1n1_knowledge  opinion_h1n1_vacc_effective  \\\n",
       "12369           3.0             1.0                          3.0   \n",
       "17593           3.0             2.0                          4.0   \n",
       "2698            3.0             1.0                          3.0   \n",
       "13754           2.0             2.0                          3.0   \n",
       "7106            2.0             1.0                          2.0   \n",
       "\n",
       "       opinion_h1n1_risk  opinion_h1n1_sick_from_vacc  \\\n",
       "12369                0.0                          3.0   \n",
       "17593                1.0                          0.0   \n",
       "2698                 0.0                          1.0   \n",
       "13754                1.0                          1.0   \n",
       "7106                 3.0                          1.0   \n",
       "\n",
       "       opinion_seas_vacc_effective  opinion_seas_risk  \\\n",
       "12369                          3.0                0.0   \n",
       "17593                          4.0                1.0   \n",
       "2698                           4.0                0.0   \n",
       "13754                          3.0                1.0   \n",
       "7106                           0.0                3.0   \n",
       "\n",
       "       opinion_seas_sick_from_vacc  age_group  education  ...  \\\n",
       "12369                          1.0        3.0        0.0  ...   \n",
       "17593                          0.0        1.0        2.0  ...   \n",
       "2698                           0.0        0.0        3.0  ...   \n",
       "13754                          1.0        3.0        2.0  ...   \n",
       "7106                           1.0        0.0        3.0  ...   \n",
       "\n",
       "       hhs_geo_region_mlyzmhmf  hhs_geo_region_oxchjgsf  \\\n",
       "12369                      0.0                      0.0   \n",
       "17593                      1.0                      0.0   \n",
       "2698                       0.0                      0.0   \n",
       "13754                      0.0                      0.0   \n",
       "7106                       0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_qufhixun  census_msa_MSA, Not Principle  City  \\\n",
       "12369                      0.0                                  0.0   \n",
       "17593                      0.0                                  1.0   \n",
       "2698                       0.0                                  0.0   \n",
       "13754                      0.0                                  0.0   \n",
       "7106                       0.0                                  1.0   \n",
       "\n",
       "       census_msa_MSA, Principle City  census_msa_Non-MSA  seasonal_vaccine_0  \\\n",
       "12369                             0.0                 1.0                 0.0   \n",
       "17593                             0.0                 0.0                 0.0   \n",
       "2698                              1.0                 0.0                 1.0   \n",
       "13754                             1.0                 0.0                 1.0   \n",
       "7106                              0.0                 0.0                 0.0   \n",
       "\n",
       "       seasonal_vaccine_1  household_adults  household_children  \n",
       "12369                 1.0               0.0                 0.0  \n",
       "17593                 1.0               1.0                 2.0  \n",
       "2698                  0.0               1.0                 2.0  \n",
       "13754                 0.0               1.0                 0.0  \n",
       "7106                  1.0               3.0                 3.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting dataframe between ordinals and categoricals\n",
    "X_test_ord =  X_test.iloc[:,np.r_[0:2,14:22,24]]\n",
    "X_test_nom = X_test.iloc[:,30:32]\n",
    "ord_cols = X_test_ord.columns\n",
    "nom_cols = X_test_nom.columns\n",
    "cols_to_drop = ord_cols.append(nom_cols)\n",
    "X_test_cat = X_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# create index arrays to use when I recreate the dataframe\n",
    "X_test_ord_index = X_test_ord.index\n",
    "X_test_cat_index = X_test_cat.index\n",
    "\n",
    "# I will convert all the columns in the dataset to string type, so I can then encode them:\n",
    "X_test_ord = X_test_ord.astype(str)\n",
    "X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "# Setting arrays back to dataframes\n",
    "X_test_enc_df = pd.DataFrame(enc.transform(X_test_ord), columns=enc_col_names,index=X_test_ord_index)\n",
    "X_test_ohe_df = pd.DataFrame(ohe.transform(X_test_cat), columns=ohe_col_names,index=X_test_cat_index)\n",
    "\n",
    "#putting the datframe back together:\n",
    "X_test_II_encoded = pd.concat([X_test_enc_df,X_test_ohe_df,X_test_nom],axis=1)\n",
    "X_test_II_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating K nearest neighbor classifier object \n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "\n",
    "# using 2-split cross-validation to score the classification:\n",
    "knn_cv_score = cross_val_score(knn, X_train_II_encoded, y_train, cv=2)\n",
    "\n",
    "# return the mean of the 5 accuracy scores:\n",
    "mean_knn_cv_score = np.mean(knn_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_knn_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model did significantly better than we would expect from random guessing (~60% better),\n",
    "and slightly better than we would expect if we simply choose the majority label (~79%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV to create additional KNN models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid:\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [5,12,20],\n",
    "    'metric'     : ['minkowski'],\n",
    "    'p'          : [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV object:\n",
    "knn_grid_search = GridSearchCV(knn, knn_param_grid, cv=2, return_train_score=True, n_jobs = -1)\n",
    "\n",
    "# Fit to the data\n",
    "knn_grid_search.fit(X_train_II_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "knn_gs_training_score = np.mean(knn_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "knn_grid_search.score(X_test_II_encoded, y_test)\n",
    "knn_gs_testing_score = np.mean(knn_grid_search.cv_results_['mean_test_score'])\n",
    "\n",
    "print(f\"Mean Training Score: {knn_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {knn_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination (to return highest score for the holdout data):\")\n",
    "knn_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a dataframe from knn_grid_search.cv_results_ dictionary:\n",
    "knn_cv_grid_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "# adding new column \n",
    "knn_cv_grid_df['score_dif'] = abs(knn_cv_grid_df['mean_train_score'] - knn_cv_grid_df['mean_test_score'])\n",
    "\n",
    "# creates new dataframe with only 'train' and 'test' scores\n",
    "knn_scores = knn_cv_grid_df.loc[:,['mean_train_score','mean_test_score','score_dif']]\n",
    "knn_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('best score: ', best_score_metric(knn_scores,knn_cv_grid_df)[0])\n",
    "print('best score difference: ', best_score_metric(knn_scores,knn_cv_grid_df)[1])\n",
    "print('best train-test combination score(auc): ', best_score_metric(knn_scores,knn_cv_grid_df)[2])\n",
    "print('best dataframe row: ',best_score_metric(knn_scores,knn_cv_grid_df)[4])\n",
    "print('best parameters: ', best_score_metric(knn_scores,knn_cv_grid_df)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_II_sample = shap.sample(X_train_II_encoded, nsamples=100, random_state=42)\n",
    "X_II_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "knn_II = KNeighborsClassifier(n_jobs = -1)\n",
    "knn_II.fit(X_train_II_encoded, y_train)\n",
    "\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "#shap.plots.waterfall(shap_values[0])\n",
    "\n",
    "# Get the model explainer object\n",
    "explainer = shap.KernelExplainer(knn_II.predict_proba, X_II_sample,n_jobs = -1)\n",
    "\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get shap values for the test data observation whose index is 0, i.e. first observation in the test set\n",
    "shap_values = explainer.shap_values(X_II_sample.iloc[0,:])\n",
    "\n",
    "# Generate a force plot for this first observation using the derived shap values\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test_II_encoded.iloc[0,:])\n",
    "\n",
    "# Generate a force plot for this first observation using the derived shap values\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test_II_encoded.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: knn.predict_proba(x)[:,1]\n",
    "med = X_train_II_encoded.median().values.reshape((1,X_train_II_encoded.shape[1]))\n",
    "explainer = shap.KernelExplainer(f, med)\n",
    "shap_values_single = explainer.shap_values(X_train_II_encoded.iloc[0,:], nsamples=1000)\n",
    "shap.force_plot(explainer.expected_value, shap_values_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating decision tree classifier object\n",
    "dec_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# using 2-split cross-validation to score the classification:\n",
    "dec_tree_cv_score = cross_val_score(dec_tree, X_train_II_encoded, y_train, cv=2)\n",
    "\n",
    "# return the mean of the 2 accuracy scores:\n",
    "mean_dec_tree_cv_score = np.mean(dec_tree_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_dec_tree_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model did significantly better than we would expect from random guessing (~50% better),\n",
    "but slightly worse than we would expect if we simply choose the majority label (~79%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV to create additional Decision Tree models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid:\n",
    "\n",
    "dec_tree_param_grid = {\n",
    "    'criterion'        : ['gini', 'entropy'],\n",
    "    'max_depth'        : [None,5,6, 7, 8],\n",
    "    'min_samples_split': [2,3,5],\n",
    "    'min_samples_leaf' : [1, 2, 3, 4, 5, 6],\n",
    "    'class_weight'     : [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV object:\n",
    "dec_tree_grid_search = GridSearchCV(dec_tree, dec_tree_param_grid, cv=2, return_train_score=True, \n",
    "                                    n_jobs = -1)\n",
    "\n",
    "# Fit to the data\n",
    "dec_tree_grid_search.fit(X_train_II_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "dec_tree_gs_training_score = np.mean(dec_tree_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "dec_tree_grid_search.score(X_test_II_encoded, y_test)\n",
    "dec_tree_gs_testing_score = np.mean(dec_tree_grid_search.cv_results_['mean_test_score'])\n",
    "\n",
    "# Print Results\n",
    "print(f\"Mean Training Score: {dec_tree_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {dec_tree_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination (to return highest score for the holdout data):\")\n",
    "dec_tree_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe from dec_tree_grid_search.cv_results_ dictionary:\n",
    "dec_tree_gs_df = pd.DataFrame(dec_tree_grid_search.cv_results_)\n",
    "\n",
    "# adding new column:\n",
    "dec_tree_gs_df['score_dif'] = abs(dec_tree_gs_df['mean_train_score'] -  dec_tree_gs_df['mean_test_score'])\n",
    "\n",
    "# creates new dataframe with only 'train','test' scores, and their difference:\n",
    "dec_tree_scores = dec_tree_gs_df.loc[:,['mean_train_score','mean_test_score','score_dif']]\n",
    "dec_tree_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score: ', best_score_metric(dec_tree_scores,dec_tree_gs_df)[0])\n",
    "print('best score difference: ', best_score_metric(dec_tree_scores,dec_tree_gs_df)[1])\n",
    "print('best train-test combination score(auc): ', best_score_metric(dec_tree_scores,dec_tree_gs_df)[2])\n",
    "print('best dataframe row: ',best_score_metric(dec_tree_scores,dec_tree_gs_df)[4])\n",
    "print('best parameters: ', best_score_metric(dec_tree_scores,dec_tree_gs_df)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest classifier object\n",
    "forest = RandomForestClassifier(n_jobs = -1,random_state=42)\n",
    "\n",
    "# using 5-split cross-validation to score the classification:\n",
    "forest_cv_score = cross_val_score(forest, X_train_II_encoded, y_train, cv=2)\n",
    "\n",
    "# return the mean of the 5 accuracy scores:\n",
    "mean_forest_cv_score = np.mean(forest_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_forest_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model did significantly better than we would expect from random guessing (~66% better),\n",
    "but slightly worse than we would expect if we simply choose the majority label (~79%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_II = RandomForestClassifier(n_jobs = -1,random_state=42)\n",
    "forest_II.fit( X_train_II_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap_values = shap.TreeExplainer(forest_II).shap_values(X_train_II_encoded,2048)\n",
    "shap.summary_plot(shap_values, X_train_II_encoded, plot_type=\"bar\",n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV to create additional Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid:\n",
    "\n",
    "forest_param_grid = {\n",
    "              'criterion'        : ['gini', 'entropy'],\n",
    "              'max_depth'        : [None, 4,5,6,8],\n",
    "              'min_samples_split': [2,3,4,6],\n",
    "              'max_features'     : [15, 20, 32,'auto'],\n",
    "             'class_weight'      : [None, 'balanced'],\n",
    "              'n_estimators'     : [100, 150]\n",
    "         \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV object:\n",
    "forest_grid_search = GridSearchCV(forest, forest_param_grid, cv=2, return_train_score=True,\n",
    "                                  n_jobs = -1)\n",
    "\n",
    "# Fit to the data\n",
    "forest_grid_search.fit(X_train_II_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "forest_gs_training_score = np.mean(forest_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "forest_grid_search.score(X_test_II_encoded, y_test)\n",
    "forest_gs_testing_score = np.mean(forest_grid_search.cv_results_['mean_test_score'])\n",
    "\n",
    "print(f\"Mean Training Score: {forest_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {forest_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination (to return highest score for the holdout data):\")\n",
    "forest_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a dataframe from forest_grid_search.cv_results_ dictionary:\n",
    "forest_cv_grid_df = pd.DataFrame(forest_grid_search.cv_results_)\n",
    "\n",
    "# adding new column:\n",
    "forest_cv_grid_df['score_dif'] = abs(forest_cv_grid_df['mean_train_score'] - forest_cv_grid_df['mean_test_score'])\n",
    "\n",
    "# creates new dataframe with only 'train','test' scores, and their difference:\n",
    "forest_scores = forest_cv_grid_df.loc[:,['mean_train_score','mean_test_score','score_dif']]\n",
    "forest_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score: ', best_score_metric(forest_scores,forest_cv_grid_df)[0])\n",
    "print('best score difference: ', best_score_metric(forest_scores,forest_cv_grid_df)[1])\n",
    "print('best train-test combination score(auc): ', best_score_metric(forest_scores,forest_cv_grid_df)[2])\n",
    "print('best dataframe row: ', best_score_metric(forest_scores,forest_cv_grid_df)[4])\n",
    "print('best parameters: ', best_score_metric(forest_scores,forest_cv_grid_df)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframes with 2 column names modified so they work with XGBoost:\n",
    "\n",
    "X_train_III_encoded = X_train_II_encoded.rename(columns={'education_< 12 Years': 'education less than 12 Years', \n",
    "                                                         'income_poverty_<= $75,000, Above Poverty':\n",
    "                                                         'income_poverty less than or = to $75000_Above Poverty'})\n",
    "X_test_III_encoded = X_test_II_encoded.rename(columns={'education_< 12 Years': 'education less than 12 Years', \n",
    "                                                         'income_poverty_<= $75,000, Above Poverty':\n",
    "                                                         'income_poverty less than or = to $75000_Above Poverty'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest classifier object\n",
    "xgboost_clf = XGBClassifier(random_state=42, n_jobs = -1)\n",
    "\n",
    "# using 2-split cross-validation to score the classification:\n",
    "xgboost_clf_cv_score = cross_val_score(xgboost_clf, X_train_III_encoded, y_train, cv=2)\n",
    "\n",
    "# return the mean of the 2 accuracy scores:\n",
    "mean_xgboost_clf_cv_score = np.mean(xgboost_clf_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_xgboost_clf_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV to create additional XGBoost Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid:\n",
    "\n",
    "xgboost_param_grid = {\n",
    "    'learning_rate': [None, .08, .1],\n",
    "    'max_depth': [None, 4, 5, 6 ],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.65, 1],\n",
    "    'min_split_loss' : [0, .5],\n",
    "    'n_estimators' : [100, 160],\n",
    "    'reg_alpha':[None, .5,]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV object:\n",
    "xgboost_clf_grid_search = GridSearchCV(xgboost_clf, xgboost_param_grid, cv=2, return_train_score=True,\n",
    "                                  n_jobs = -1)\n",
    "\n",
    "# Fit to the data\n",
    "xgboost_clf_grid_search.fit(X_train_III_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "xgboost_clf_gs_training_score = np.mean(xgboost_clf_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "xgboost_clf_grid_search.score(X_test_III_encoded, y_test)\n",
    "xgboost_clf_gs_testing_score = np.mean(xgboost_clf_grid_search.cv_results_['mean_test_score'])\n",
    "\n",
    "print(f\"Mean Training Score: {xgboost_clf_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {xgboost_clf_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination (to return highest score for the holdout data):\")\n",
    "xgboost_clf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe from xgboost_clf_grid_search.cv_results_ dictionary:\n",
    "xgboost_clf_grid_df = pd.DataFrame(xgboost_clf_grid_search.cv_results_)\n",
    "\n",
    "# adding new column:\n",
    "xgboost_clf_grid_df['score_dif'] = abs(xgboost_clf_grid_df['mean_train_score'] - \n",
    "                                       xgboost_clf_grid_df['mean_test_score'])\n",
    "\n",
    "# creates new dataframe with only 'train','test' scores, and their difference:\n",
    "xgboost_scores = xgboost_clf_grid_df.loc[:,['mean_train_score','mean_test_score','score_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score: ', best_score_metric(xgboost_scores,xgboost_clf_grid_df)[0])\n",
    "print('best score difference: ', best_score_metric(xgboost_scores,xgboost_clf_grid_df)[1])\n",
    "print('best train-test combination score(auc): ', best_score_metric(xgboost_scores,xgboost_clf_grid_df)[2])\n",
    "print('best dataframe row: ', best_score_metric(xgboost_scores,xgboost_clf_grid_df)[4])\n",
    "print('best parameters: ', best_score_metric(xgboost_scores,xgboost_clf_grid_df)[3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
